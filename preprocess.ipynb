{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e679fc0e",
   "metadata": {},
   "source": [
    "# Data Preprocessing Notebook\n",
    "\n",
    "This notebook preprocesses two datasets:\n",
    "\n",
    "- **NASDAQ news data**: Converts date strings to UTC and saves the result as a Parquet file.\n",
    "- **Full stock price history**: Standardizes date columns, converts dates to UTC, and saves each file as a Parquet file.\n",
    "\n",
    "Make sure the required CSV files are available in the specified folders before running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2e9cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta, datetime\n",
    "import polars as pl\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a20d0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "input_file = 'nasdaq_external_data.csv'\n",
    "output_file = 'nasdaq_small.csv'\n",
    "n = 100\n",
    "\n",
    "# Read the CSV file using Polars\n",
    "df = pl.read_csv(input_file)\n",
    "\n",
    "df_head = df.head(n)\n",
    "df_head.write_csv(output_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d65a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_utc(time_str):\n",
    "    if isinstance(time_str, float) or not isinstance(time_str, str):\n",
    "        return \"Invalid date format\"\n",
    "        \n",
    "    if \" EDT\" in time_str:\n",
    "        time_str_cleaned = time_str.replace(\" EDT\", \"\")\n",
    "        offset = timedelta(hours=-4)\n",
    "    elif \" EST\" in time_str:\n",
    "        time_str_cleaned = time_str.replace(\" EST\", \"\")\n",
    "        offset = timedelta(hours=-5)\n",
    "    else:\n",
    "        offset = timedelta(hours=0)\n",
    "        time_str_cleaned = time_str\n",
    "\n",
    "    formats = [\n",
    "        '%B %d, %Y — %I:%M %p',  # e.g., \"September 12, 2023 — 06:15 pm\"\n",
    "        '%b %d, %Y %I:%M%p',      # e.g., \"Nov 14, 2023 7:35AM\"\n",
    "        '%d-%b-%y',              # e.g., \"6-Jan-22\"\n",
    "        '%Y-%m-%d',              # e.g., \"2021-4-5\"\n",
    "        '%Y/%m/%d',              # e.g., \"2021/4/5\"\n",
    "        '%b %d, %Y',             # e.g., \"DEC 7, 2023\"\n",
    "        '%Y-%m-%d %H:%M:%S'      # e.g., \"2023-12-07 16:30:00\"\n",
    "    ]\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            dt = datetime.strptime(time_str_cleaned, fmt)\n",
    "            # For the '%d-%b-%y' format, ignore offset adjustment\n",
    "            if fmt == '%d-%b-%y':\n",
    "                offset = timedelta(hours=0)\n",
    "            dt_utc = dt + offset\n",
    "            # Return ISO formatted string without the \" UTC\" suffix\n",
    "            return dt_utc.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return \"Invalid date format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65eb75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nasdaq_data_parquet(file_path, saving_path):\n",
    "    print('Processing NASDAQ news data...')\n",
    "    try:\n",
    "        # Use lazy reading for memory efficiency\n",
    "        df = pl.scan_csv(file_path, infer_schema_length=50000, schema_overrides={\"Article\": pl.Utf8})\n",
    "        \n",
    "        print(\"scan done\")\n",
    "        # Drop the Unnamed: 0 column if it exists (using lazy schema collection)\n",
    "        if \"Unnamed: 0\" in df.collect_schema().names():\n",
    "            df = df.drop(\"Unnamed: 0\")\n",
    "\n",
    "        print(\"drop column\")\n",
    "        \n",
    "        # Convert the Date column to UTC (using the custom function)\n",
    "        # Collect only the Date column to minimize overhead\n",
    "        date_df = df.select(\"Date\").collect()\n",
    "        converted_dates = date_df.with_columns([\n",
    "            pl.col(\"Date\").map_elements(convert_to_utc, return_dtype=pl.Utf8).alias(\"Date\")\n",
    "        ])\n",
    "\n",
    "        print(\"strp time\")\n",
    "        \n",
    "        # Replace the original Date column with the converted dates (still as strings)\n",
    "        df = df.with_columns([\n",
    "            pl.lit(converted_dates.get_column(\"Date\")).alias(\"Date\")\n",
    "        ])\n",
    "        \n",
    "        print(\"date\")\n",
    "        \n",
    "        # Since our dates are now in ISO format (\"YYYY-MM-DD HH:MM:SS\"),\n",
    "        # we can sort them as strings and get correct chronological order.\n",
    "        df = df.sort(\"Date\", descending=True)\n",
    "        \n",
    "        # Ensure saving path exists\n",
    "        os.makedirs(saving_path, exist_ok=True)\n",
    "        output_file = os.path.join(saving_path, os.path.basename(file_path).replace('.csv', '.parquet'))\n",
    "        \n",
    "        print(\"executing lazy query\")\n",
    "        # Execute the lazy query and write directly to parquet\n",
    "        df.collect().write_parquet(output_file)\n",
    "        print('Done processing NASDAQ news data. Saved to:', output_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing NASDAQ news data: {str(e)}\")\n",
    "        raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb4b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stock_history_parquet(folder_path, saving_path):\n",
    "    os.makedirs(saving_path, exist_ok=True)\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    for csv_file in csv_files:\n",
    "        print(f'Processing {csv_file}...')\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        \n",
    "        try:\n",
    "            df = pl.read_csv(file_path)\n",
    "\n",
    "            # Lowercase all column names\n",
    "            for col in df.columns:\n",
    "                df = df.rename({col: col.lower()})\n",
    "\n",
    "            # Identify and standardize the date column (rename to \"Date\")\n",
    "            date_columns = [\"datetime\", \"date\", \"time\", \"timestamp\"]\n",
    "            for col in date_columns:\n",
    "                if col.lower() in df.columns:\n",
    "                    df = df.rename({col: \"Date\"})\n",
    "                    break\n",
    "            \n",
    "            if \"Date\" not in df.columns:\n",
    "                print(f\"No date column found in {csv_file}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            df = df.with_columns([\n",
    "                pl.col(\"Date\").map_elements(convert_to_utc, return_dtype=pl.Utf8).alias(\"Date\")\n",
    "            ])\n",
    "            \n",
    "            # Sort by date descending\n",
    "            df = df.sort(\"Date\", descending=True)\n",
    "            \n",
    "            output_file = os.path.join(saving_path, csv_file.replace('.csv', '.parquet'))\n",
    "            df.write_parquet(output_file)\n",
    "            print(f'Done processing {csv_file}. Saved to: {output_file}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b41155",
   "metadata": {},
   "source": [
    "## Run the Preprocessing Functions\n",
    "\n",
    "Adjust the file paths as needed before running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a9c0ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NASDAQ news data...\n",
      "scan done\n",
      "drop column\n",
      "strp time\n",
      "date\n",
      "executing lazy query\n",
      "Done processing NASDAQ news data. Saved to: nasdaq_data_preprocessed\\nasdaq_external_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Define paths for the input files and output folders\n",
    "# For NASDAQ news data:\n",
    "nasdaq_file_path = 'nasdaq_external_data.csv'\n",
    "nasdaq_saving_path = 'nasdaq_data_preprocessed'\n",
    "\n",
    "# Process the NASDAQ news data and save as Parquet\n",
    "process_nasdaq_data_parquet(nasdaq_file_path, nasdaq_saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing full stock price history...\n",
      "Using 11 processes for parallel processing...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# For full stock price history:\n",
    "full_history_folder_path = 'full_history'\n",
    "full_history_saving_path = 'full_history_preprocessed'\n",
    "\n",
    "# Process the full stock history and save each file as Parquet\n",
    "process_stock_history_parquet(full_history_folder_path, full_history_saving_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
